package edu.stthomas.gps;

import java.io.IOException;
import java.net.URI;

import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.conf.Configuration;


public class NeuronGraphReducer extends Reducer<IntWritable, NeuronWritable, IntWritable, MultiWritableWrapper> {

	private MultiWritableWrapper multi_writable = new MultiWritableWrapper(); // value
	private NeuronWritable neuron = new NeuronWritable();
	private SequenceFile.Reader reader;
	private IntWritable graphPartitonKey = new IntWritable();
	private MultiWritableWrapper graphPartitonValue = new MultiWritableWrapper();
	private boolean firstCalled;
	
	private enum Test {
		weights_count, neuron_count, test,
	}
	
	/*
	 * In setup, we open the file containing the graph partition
	 * corresponding to the intermediate keys to be processed by the reducer.
	 */
	@Override
	public void setup(Context context) 
			throws IOException, InterruptedException {
		firstCalled = false;
	}
	
	private void openGraphPartition(int id, Context context) 
			throws IOException, InterruptedException {
		/*
		 * Compute the number of current graph partition 
		 */
		int reducer_num = (id - 1) / NeuronIDRangePartitioner.NumOfNeuronsPerPartition;
		
		/*
		 * Open the graph partition which is a sequence file generated by reducer.
		 */
		String iteration_num = context.getJobName().split(":")[1];
		String uri;
		if (iteration_num.equals("1")) { 
			// First iteration, input to the job is "neuron_graph_input"
			uri = "neuron_graph_input_1000" + "/" + "part-r-0000" + reducer_num;
		} else {
			// After the first iteration, input for the job is the output of the previous job.
			uri = "neuron_graph_output" + (Integer.parseInt(iteration_num)-1) + "/" + "part-r-0000" + reducer_num;
		}
		Configuration conf = new Configuration();
		FileSystem fs = FileSystem.get(URI.create(uri), conf);
		Path path = new Path(uri);
		reader = new SequenceFile.Reader(fs, path, conf);
	}
	
	private boolean isNeuron(NeuronWritable value) {
		if (value.getTypeOfValue() == 'W') {
			return false;
		} else {
			return true;
		}
	}
	
	public void reduce(IntWritable key, Iterable<NeuronWritable> values, Context context) 
			throws IOException, InterruptedException {
		
		if (this.firstCalled == false) {
			this.openGraphPartition(key.get(), context);
			this.firstCalled = true;
		}
		
		float weight_sum = 0;
			
		while (reader.next(graphPartitonKey, graphPartitonValue)) {
			if (key.equals(graphPartitonKey)) {
				break;
			} else {
				context.getCounter(Test.test).increment(1);
				context.write(graphPartitonKey, graphPartitonValue);
			}
		}
		
		for (NeuronWritable value : values) {
			if (!isNeuron(value)) {
				context.getCounter(Test.weights_count).increment(1);
				weight_sum += value.getWeight();
			} else { // Recover the neuron structure.
				//neuron = value;
				graphPartitonValue.getNeuronWritable().fired = value.fired;
				graphPartitonValue.getNeuronWritable().potential = value.potential;
				graphPartitonValue.getNeuronWritable().recovery = value.recovery;
				graphPartitonValue.getNeuronWritable().time = value.time;
				context.getCounter(Test.neuron_count).increment(1);
			}
		}
		
		// Update synaptic summation for the next iteration. This is the 
		// only information needs to be updated.
		graphPartitonValue.getNeuronWritable().synaptic_sum = weight_sum;
		//graphPartitonValue.setNeuronWritable(neuron);
		
		//graphPartitonValue.getNeuronWritable().fired = neuron.fired;
		//graphPartitonValue.getNeuronWritable().potential = neuron.potential;
		//graphPartitonValue.getNeuronWritable().recovery = neuron.recovery;
		//graphPartitonValue.getNeuronWritable().time = neuron.time;
		
		context.write(key, graphPartitonValue);
		//context.write(key, neuron);
	}
	
	@Override
	public void cleanup(Context context) 
			throws IOException, InterruptedException {
		reader.close();
	}
}
